# Counterfactual Regret Minimization (CFR)

CFR is the leading algorithm for solving large **Extensive-Form Games with Imperfect Information** (e.g., Poker). It iteratively converges to a Nash Equilibrium in two-player zero-sum games.

## Defines & Notation
-   **History ($h$)**: A sequence of actions from the root.
-   **Information Set ($I$)**: A set of histories indistinguishable to the player acting at those histories.
-   **Strategy ($\sigma$)**: Probability distribution over actions at each $I$.
-   **Counterfactual Value ($v_i(\sigma, I)$)**: The expected utility for player $i$ if they played to reach $I$ and then followed $\sigma$.

## The Core Algorithm (Vanilla CFR)

### 1. Regret Calculation
For each information set $I$ and action $a$, we compute the **immediate counterfactual regret**:
$$R^T(I, a) = \sum_{t=1}^T (u_i(\sigma^t_{I \to a}, I) - u_i(\sigma^t, I))$$
where:
-   $u_i(\sigma^t, I)$ is the utility of the current strategy.
-   $u_i(\sigma^t_{I \to a}, I)$ is the utility if we *forced* action $a$ at $I$ but played $\sigma^t$ everywhere else.

### 2. Regret Matching (Strategy Update)
The strategy for the next iteration $T+1$ is chosen proportional to the positive cumulative regret:
$$\sigma^{T+1}(I, a) = \frac{R^{T,+}(I, a)}{\sum_{a'} R^{T,+}(I, a')}$$
where $x^+ = \max(x, 0)$. If all regrets are non-positive, play a uniform random strategy.

### 3. Averaging
The *current* strategy $\sigma^T$ might oscillate. The **average strategy** $\bar{\sigma}^T$ (weighted by iteration number) is the one that converges to the Nash Equilibrium.

## Key Variants

### CFR+ (Solving Video Games / Poker)
The state-of-the-art variant used in solvers like Libratus.
1.  **Regret Floor**: Cumulative regrets are floored at 0 ($R(I,a) \leftarrow \max(R(I,a), 0)$ at each step).
2.  **Alternating Updates**: Update P1, then P2 (instead of simultaneous).
3.  **Linear Averaging**: Weights recent iterations more heavily ($w_t = t$).

### MCCFR (Monte Carlo CFR)
Traversing the full game tree is impossible for large games. MCCFR samples parts of the tree.
-   **External Sampling**: Sample chance nodes and opponent actions, but traverse *all* of your own actions at visited infosets. (Lower variance).
-   **Outcome Sampling**: Sample a single path down the tree. (Fastest, high variance).

### Deep CFR
Uses Deep Neural Networks to approximate the regret values and potentially the strategy.
-   Allows scaling to games too large for tabular (matrix) CFR.
-   Trains a "Value Network" on the counterfactual regrets generated by MCCFR samples.

## Algorithm Complexity
-   Time per iteration: $O(|I|)$ (linear in game size).
-   Convergence: $O(1/\sqrt{T})$ to Nash Equilibrium.

## Interview Questions
-   **"Why Regret Matching?"**: It guarantees no-regret learning, which in self-play implies convergence to NE in zero-sum games (Folk Theorem).
-   **"Difference between CFR and RL?"**: RL (usually) learns a best response to a fixed environment. CFR learns a strategy that is unexploitable by *any* opponent by minimizing regret against *all* possible opponent strategies simultaneously via self-play.
