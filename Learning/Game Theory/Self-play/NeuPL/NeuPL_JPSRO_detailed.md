# NeuPL-JPSRO â€” Detailed Implementation Notes

> **Quick overview**: [[NeuPL_JPSRO]]

## Paper

**Title**: Neural Population Learning beyond Symmetric Zero-sum Games

**Authors**: Siqi Liu, Luke Marris, Marc Lanctot, Nicolas Heess (DeepMind)

**Year**: 2024

**ArXiv**: [2401.05133](https://arxiv.org/abs/2401.05133)

## Core Algorithm (Algorithm 2)

```
function NeuPL-JPSRO(game, Îµ):
    # Initialization
    for each player p:
        Initialize embedding Î½â‚šâ° âˆˆ â„áµˆ
        ğ’±â‚š = {Î½â‚šâ°}
    Initialize conditional policy Î Î¸(Â·|s,Î½)
    Initialize payoff estimator Ïˆw(Î½â‚,...,Î½â‚™)
    Ïƒâ° = CCE_Solver(evaluate_payoffs(ğ’±, Î Î¸))

    for iteration t = 1 to convergence:
        Î¸Ì‚ = Î¸       # Freeze reference parameters
        ğ’±Ì‚ = ğ’±       # Freeze embeddings

        for each player p:
            # Step 1: Best Response (against frozen co-players)
            Ï€â‚šáµ— = BR(player=p, Ïƒâ‚‹â‚šáµ—â»Â¹, Î Î¸Ì‚, ğ’±Ì‚)

            # Step 2: Distill BR into population
            Î½â‚šáµ— = new_embedding()
            min KL(Ï€â‚šáµ— || Î Î¸(Â·|Â·,Î½â‚šáµ—))

            # Step 3: Regularize existing strategies
            for Î½ âˆˆ ğ’±â‚š:
                min KL(Î Î¸Ì‚(Â·|Â·,Î½Ì‚) || Î Î¸(Â·|Â·,Î½))

            ğ’±â‚š = ğ’±â‚š âˆª {Î½â‚šáµ—}

        # Step 4: Update meta-game
        Gáµ— = evaluate_payoffs(ğ’±, Î Î¸, Ïˆw)
        Ïƒáµ— = CCE_Solver(Gáµ—)

        if max_p CCE_Gap_p(Gáµ—, Ïƒáµ—) < Îµ: break

    return Î Î¸, ğ’±, Ïƒáµ—
```

## Network Architecture

### Policy Network (Î Î¸)

```
Input: (observation s, strategy embedding Î½)
    â†“
Observation Encoder (shared)
    Conv/MLP â†’ encoded_obs
    â†“
Recurrent Memory (shared, LSTM 128-256 units)
    â†’ memory_state
    â†“
Conditioning (FiLM, concat, or attention)
    [encoded_obs, memory_state, Î½] â†’ features
    â†“
Policy Head â†’ Ï€(a|s,Î½) = Softmax(logits)
```

### Best-Response Head (Î Ï•)

```
Input: (observation s, co-player mixed-strategy encoding)
    â†“
Reuse: Encoder + Memory from Î Î¸ (frozen or trainable)
    â†“
Mixed-Strategy Encoding (top-k=96 joint strategies)
    g(ğ’±, Ïƒâ‚‹â‚š) = Î£ Ïƒâ‚‹â‚š(aâ‚‹â‚š) Â· f(Î½ embeddings)
    â†“
BR Policy Head (separate) â†’ Ï€Ï•(a|s,Ïƒâ‚‹â‚š)
```

### Payoff Estimator (Ïˆw)

```
Input: Joint embeddings (Î½â‚,...,Î½â‚™)
    â†’ Concatenation or symmetric encoding
    â†’ MLP
    â†’ Output: [payoff_p1, ..., payoff_pn] âˆˆ â„â¿
```

## Key Design Decisions

### Iterative vs Concurrent Training

| NeuPL-JPSRO (Iterative) | Original NeuPL (Concurrent) |
|---|---|
| Freeze Î¸Ì‚, train BR against stationary co-players | Continuously train all policies |
| Ensures JPSRO convergence guarantees | Co-players are moving targets |
| More expensive per iteration | More sample-efficient |

### Reference Parameter Freezing

```python
Î¸Ì‚ = Î¸   # Ensures co-player stationarity
ğ’±Ì‚ = ğ’±   # Prevents "moving targets" problem
```

### Distillation + Regularization

```python
# Distill BR into population
loss_distill = KL(Ï€â‚šáµ—(Â·|s) || Î Î¸(Â·|s,Î½â‚šáµ—))

# Prevent catastrophic forgetting
loss_reg = KL(Î Î¸Ì‚(Â·|s,Î½Ì‚) || Î Î¸(Â·|s,Î½))  # for each existing Î½
```

## CCE Solver (Linear Program)

```python
import cvxpy as cp

def solve_CCE(payoff_tensor, num_players, actions_per_player):
    joint_actions = list(itertools.product(
        *[range(a) for a in actions_per_player]
    ))
    sigma = cp.Variable(len(joint_actions))
    constraints = [sigma >= 0, cp.sum(sigma) == 1]

    # Incentive constraints for each player
    for player in range(num_players):
        for ap in range(actions_per_player[player]):
            for ap_dev in range(actions_per_player[player]):
                if ap == ap_dev: continue
                payoff_comply = sum(...)  # expected payoff following Ïƒ
                payoff_deviate = sum(...)  # expected payoff deviating
                constraints.append(payoff_comply >= payoff_deviate)

    problem = cp.Problem(cp.Minimize(0), constraints)
    problem.solve()
    return sigma.value
```

## Convergence

**Theorem 3.2**: Under exact distillation and regularization, NeuPL-JPSRO converges to a normal-form CCE.

**In practice**: Distillation/regularization are approximate â†’ bounded error, but empirically still converges to near-CCE.

## Hyperparameters

| Parameter | Value Range |
|-----------|-------------|
| Strategy embedding dim | 64â€“256 |
| Encoder hidden units | 128â€“512 |
| LSTM units | 128â€“256 |
| LR (policy) | 1e-4 to 1e-3 |
| LR (payoff estimator) | 1e-4 to 1e-3 |
| Entropy regularization Î± | 0.001â€“0.01 |
| KL distillation weight | 1.0â€“10.0 |
| KL regularization weight | 1.0â€“10.0 |
| Top-k for mixed-strategy | k=96 |
| CCE solver | LP (CVXPY / OSQP) |

## Evaluation Metrics

1. **CCE Gap**: Î´(Ïƒ) = Î£â‚š max(0, max_{a'â‚š}[E_{aâ‚‹â‚š~Ïƒâ‚‹â‚š} Gâ‚š(a'â‚š,aâ‚‹â‚š) - E_{a~Ïƒ} Gâ‚š(a)])
2. **Exploitability**: Via independent RL exploiters
3. **Payoff Estimator Accuracy**: MSE vs actual rollouts
4. **Policy Diversity**: KL divergence between population members

## Benchmark Tasks

| Task | Players | Type | Key Challenge |
|------|---------|------|---------------|
| OpenSpiel games (6) | 2â€“3 | Various | Analytical CCE verification |
| MuJoCo Cheetah-Run | 2 | Cooperative | Coordinated motor control |
| Capture-the-Flag | 4 (2v2) | Mixed | Partial obs, sparse rewards, teams |

## References

- [NeuPL beyond Symmetric Zero-sum (Liu et al., 2024)](https://arxiv.org/abs/2401.05133)
- [JPSRO (Marris et al., ICML 2021)](https://arxiv.org/abs/2106.09435)
- [NeuPL (Liu et al., ICLR 2022)](https://arxiv.org/abs/2202.07415)

## Related

- [[NeuPL_JPSRO]] â€” Quick overview
- [[NeuPL]] / [[NeuPL_detailed]] â€” Single-network foundation
- [[JPSRO]] / [[JPSRO_detailed]] â€” CCE convergence backbone
- [[Simplex_NeuPL]] â€” Mixture-optimal (symmetric ZS only)
